# 🚀 Criação de Pipeline no Azure

## 🧠 Objetivo
O objetivo deste projeto foi criar um pipeline utilizando ferramentas do Azure, com foco na transformação e armazenamento de arquivos, especialmente na conversão de arquivos CSV para o formato Parquet.

## 📝 Processo
1. Pesquisa e estudo das ferramentas disponíveis no ecossistema Azure.
2. Definição da ordem de criação e integração entre os componentes.
3. Execução prática do pipeline, validando cada etapa do processo.

## ⚙️ Ferramentas Utilizadas
- **Azure Data Factory**: Orquestração do pipeline de dados.
- **Azure Blob Storage**: Armazenamento dos arquivos de entrada e saída.
- **Azure Synapse / Data Lake** *(se aplicável)*: Armazenamento e análise dos dados transformados.

## 📷 Prints
![Print 1](https://github.com/user-attachments/assets/05e41e80-128e-46ee-b732-3a0f3db75320)
![Print 2](https://github.com/user-attachments/assets/cfce3f6d-3008-4800-a7cb-7aa5aaff4a07)
![Print 3](https://github.com/user-attachments/assets/3f9d6e00-b688-4f24-b531-0f9914787c35)
![Print 4](https://github.com/user-attachments/assets/78ef6050-d7bc-4297-9ccb-bf780e25fe0e)
![Print 5](https://github.com/user-attachments/assets/1b80dad3-2451-4402-998b-0a8d6a769fad)
![Print 6](https://github.com/user-attachments/assets/e93c9a8a-c8c5-4b5a-8789-e4c0e0148ef5)

## 💡 Insights
- A conversão de CSV para Parquet reduz significativamente o tamanho dos arquivos e melhora a performance em consultas analíticas.
- O uso de pipelines facilita a automação e reusabilidade dos processos de dados.

## 🔍 Possibilidades Futuras
- Integração com bancos de dados relacionais para ingestão de dados.
- Agendamento automático de execuções recorrentes.
- Suporte a outros formatos como JSON e Avro.

## 📚 Aprendizado
- Domínio prático de ferramentas do Azure para engenharia de dados.
- Compreensão sobre formatos de dados e suas vantagens.
- Aplicação de boas práticas no tratamento e transformação de dados em nuvem.







